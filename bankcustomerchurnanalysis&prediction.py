# -*- coding: utf-8 -*-
"""BankCustomerChurnAnalysis&Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NDMt3iTLH7vnRPvv6h-Vmfx14AVRQN-1

## About dataset

This dataset is for ABC Multinational bank with following columns:

- customer_id, unused variable.
- credit_score, used as input.
- country, used as input.
- gender, used as input.
- age, used as input.
- tenure, used as input.
- balance, used as input.
- products_number, used as input.
- credit_card, used as input.
- active_member, used as input.
- estimated_salary, used as input.
- churn, used as the target. 1 if the client has left the bank during some period or 0 if he/she has not.

Aim is to Predict the Customer Churn for ABC Bank.

Importing required libraries
"""

!pip install opendatasets --upgrade --quiet
import missingno as mno
import opendatasets as od
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
from imblearn.over_sampling import SMOTE
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

"""Using 'opendatasets' to download the dataset from kaggle"""

url = 'https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset'
od.download(url)

"""## Data preparation and cleaning"""

filename = "/content/bank-customer-churn-dataset/Bank Customer Churn Prediction.csv"

"""Loading the file using pandas"""

df = pd.read_csv(filename)

"""Looking at the first 5 records of the data"""

df.head()

"""Looking at 5 random records from the data"""

df.sample(5)

"""Analyzing various attributes of the data"""

df.shape

df.columns

df.info()

df.describe().T

df.nunique()

"""Using 'missingno' to visualize any missing vaues in the data"""

colors=sns.color_palette("tab20")
mno.bar(df , figsize=(12,6), fontsize=8, color=colors)
plt.show()

"""There are no missing values

Checking for duplicates
"""

df.duplicated().sum()

"""There are no duplicates

Checking for data imbalance
"""

df['churn'].value_counts()

"""Data is highly imbalanced

Creating a copy of the base data for manupulation and processing
"""

df_cp = df.copy()

"""Removing 'customer_id' since it is not required for processing"""

df_cp = df_cp.drop('customer_id',axis=1)
df_cp.info()

"""## Exploratory data analysis and visualization

Analyzing the correlation heatmap
"""

c_map=sns.color_palette("rocket_r", as_cmap=True)
correlation = df_cp.corr(numeric_only=True).round(2)
plt.figure(figsize = (12,8))
sns.heatmap(correlation, annot = True, cmap = c_map)
plt.show()

abs(correlation['churn']).sort_values(ascending=False)

"""'age', 'active_member', 'balance' show a relatively stronger correlation with the outcome of churn

Plotting pie chart showing percentage of churned and retained customers
"""

labels = 'Churned', 'Retained'
sizes = [df_cp.churn[df_cp['churn']==1].count(), df_cp.churn[df_cp['churn']==0].count()]
explode = (0, 0.1)
plt.figure(figsize=(7,7))
plt.pie(sizes, explode=explode, labels=labels, textprops={'fontsize': 10}, autopct='%1.1f%%', shadow=True, startangle=90, colors=colors)
plt.show()

"""Analyzing data using histplots"""

plt.figure(figsize=(12,20))

plt.subplot(4,1,1)
sns.histplot(df_cp, x='credit_score', hue='churn', kde=True, palette=[colors[0], colors[2]])
plt.xlabel('credit_score', fontsize=12, color=colors[0])
plt.ylabel('count', fontsize=12, color=colors[0])

plt.subplot(4,1,2)
sns.histplot(df_cp, x='age', hue='churn', kde=True, palette=[colors[0], colors[2]])
plt.xlabel('age', fontsize=12, color=colors[0])
plt.ylabel('count', fontsize=12, color=colors[0])

plt.subplot(4,1,3)
sns.histplot(df_cp, x='balance', hue='churn', kde=True, palette=[colors[0], colors[2]])
plt.xlabel('balance', fontsize=12, color=colors[0])
plt.ylabel('count', fontsize=12, color=colors[0])

plt.subplot(4,1,4)
sns.histplot(df_cp, x='estimated_salary', hue='churn', kde=True, palette=[colors[0], colors[2]])
plt.xlabel('estimated_salary', fontsize=12, color=colors[0])
plt.ylabel('count', fontsize=12, color=colors[0])

plt.show()

"""Analyzing data using countplots"""

plt.figure(figsize=(12,10))

plt.subplot(2,2,1)
cplot = sns.countplot(data=df_cp, x='country', hue='churn', order=df_cp['country'].value_counts().index, palette=colors)
plt.xlabel('country', fontsize=12, color=colors[0])
plt.ylabel('count', fontsize=12, color=colors[0])
for x in cplot.patches:
  cplot.text (x.get_x() + x.get_width()  / 2, x.get_height()+ 0.75, x.get_height(), horizontalalignment='center', fontsize = 10)

plt.subplot(2,2,2)
cplot = sns.countplot(data=df_cp, x='gender', hue='churn', order=df_cp['gender'].value_counts().index, palette=colors)
plt.xlabel('gender', fontsize=12, color=colors[0])
plt.ylabel('count', fontsize=12, color=colors[0])
for x in cplot.patches:
  cplot.text (x.get_x() + x.get_width()  / 2, x.get_height()+ 0.75, x.get_height(), horizontalalignment='center', fontsize = 10)

plt.subplot(2,2,3)
cplot = sns.countplot(data=df_cp, x='credit_card', hue='churn', order=df_cp['credit_card'].value_counts().index, palette=colors)
plt.xlabel('credit_card', fontsize=12, color=colors[0])
plt.ylabel('count', fontsize=12, color=colors[0])
for x in cplot.patches:
  cplot.text (x.get_x() + x.get_width()  / 2, x.get_height()+ 0.75, x.get_height(), horizontalalignment='center', fontsize = 10)

plt.subplot(2,2,4)
cplot = sns.countplot(data=df_cp, x='active_member', hue='churn', order=df_cp['active_member'].value_counts().index, palette=colors)
plt.xlabel('active_member', fontsize=12, color=colors[0])
plt.ylabel('count', fontsize=12, color=colors[0])
for x in cplot.patches:
  cplot.text (x.get_x() + x.get_width()  / 2, x.get_height()+ 0.75, x.get_height(), horizontalalignment='center', fontsize = 10)

plt.show()

"""- Germany has highest number of churners
- Female churners > Male churners
- Most churners are not active members
"""

plt.figure(figsize=(14,5))

cplot = sns.countplot(data=df_cp, x='products_number', hue='churn', palette=colors)
plt.xlabel('products_number', fontsize=12, color=colors[0])
plt.ylabel('count', fontsize=12, color=colors[0])
for x in cplot.patches:
  cplot.text (x.get_x() + x.get_width()  / 2, x.get_height()+ 0.75, x.get_height(), horizontalalignment='center', fontsize = 10)

plt.show()

"""- Most customers have only 1 or 2 products
- Most of the non-churners have 2 products
- Most of the churners have 1 product
- Customers having 3,4 products are mostly churners
"""

plt.figure(figsize=(14,5))

cplot = sns.countplot(data=df_cp, x='tenure', hue='churn', palette=colors)
plt.xlabel('tenure', fontsize=12, color=colors[0])
plt.ylabel('count', fontsize=12, color=colors[0])
for x in cplot.patches:
  cplot.text (x.get_x() + x.get_width()  / 2, x.get_height()+ 0.75, x.get_height(), horizontalalignment='center', fontsize = 9)

plt.show()

"""- Most churners have a tenure of 1 year
- Comparatively few customers have a tenure of 0 or 10 years

Analyzing data using scatterplots
"""

plt.figure(figsize=(10,5))

sns.scatterplot(data=df_cp, x='estimated_salary', y='credit_score', hue='churn', palette=[colors[0], colors[2]])
plt.xlabel('estimated_salary', fontsize=12, color=colors[0])
plt.ylabel('credit_score', fontsize=12, color=colors[0])
plt.xticks(rotation=45)

plt.show()

"""Customers with credit score < 400 are mostly churners"""

plt.figure(figsize=(10,5))

sns.scatterplot(data=df_cp, x='age', y='estimated_salary', hue='churn', palette=[colors[0], colors[2]])
plt.xlabel('age', fontsize=12, color=colors[0])
plt.ylabel('estimate_salary', fontsize=12, color=colors[0])

plt.show()

"""Most churners lie within the age group of 50-60 years

## Data preprocessing
"""

dfEnc=df_cp.copy()

"""Encoding 'gender', 'country'"""

dfEnc["gender"] = LabelEncoder().fit_transform(dfEnc["gender"])
dfEnc["country"] = LabelEncoder().fit_transform(dfEnc["country"])

"""Looking at the first 5 records of the data"""

dfEnc.head()

"""Seperating dependent and independent variables"""

X=dfEnc.drop('churn',axis=1)
X.head()

Y=dfEnc['churn']
Y.head()

"""Handling imbalanced data with SMOTE"""

X_bal,Y_bal=SMOTE().fit_resample(X,Y)

Y_bal.value_counts()

"""Train-test split"""

X_train,X_test,Y_train,Y_test=train_test_split(X_bal,Y_bal,test_size=0.3)

"""## Model building

Building the classifiers
"""

classifiers = [LogisticRegression(), DecisionTreeClassifier(), svm.SVC(), KNeighborsClassifier(), RandomForestClassifier()]
classifier_names = ['Logistic Regression', 'Decision tree classifier', 'Support vector classifier', 'K neighbors classifier', 'Random forest classifier']

results = []
for i in range(len(classifiers)):
    model = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', classifiers[i])
    ])
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracy = accuracy_score(Y_test, Y_pred)
    precision = precision_score(Y_test, Y_pred)
    recall = recall_score(Y_test, Y_pred)
    f1 = f1_score(Y_test, Y_pred)
    roc_auc = roc_auc_score(Y_test, Y_pred)
    results.append([classifier_names[i], accuracy, precision, recall, f1, roc_auc])
    print(classifier_names[i] + ':-\n')
    cf_matrix = confusion_matrix(Y_test, Y_pred)
    group_names = ['True Neg','False Pos','False Neg','True Pos']
    group_counts = ["{0:0.0f}".format(value) for value in cf_matrix.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]
    labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize=(4,3))
    ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap=c_map)
    ax.set_xlabel('\nPredicted Values')
    ax.set_ylabel('Actual Values ')
    ax.xaxis.set_ticklabels(['False','True'])
    ax.yaxis.set_ticklabels(['False','True'])
    plt.show()
    print('\n')
    print(classification_report(Y_test,Y_pred))
    print('\n\n')

"""Comparing performance of classifiers"""

results_df = pd.DataFrame(results, columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC score'])
display(results_df)

"""From this, we can conclude that random forest classifier is the best model for our dataset

Performing hyperparameter tuning for random forest classifier using 'RandomizedSearchCV'
"""

n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 20)]
max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]
max_depth.append(None)
max_features = [None, 'sqrt', 'log2']
min_samples_split = [2, 3, 5, 7, 11]
min_samples_leaf = [1, 2, 3, 5, 7, 11]
criterion = ['gini', 'entropy', 'log_loss']

param_grid = {'n_estimators': n_estimators,
               'max_depth': max_depth,
               'max_features': max_features,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'criterion': criterion}

scaler = StandardScaler()
X_train_sc = scaler.fit_transform(X_train)
X_test_sc = scaler.transform(X_test)
model = RandomForestClassifier()
rf_RandomGrid = RandomizedSearchCV(estimator = model, param_distributions = param_grid, cv = 3, n_jobs = -1)
rf_RandomGrid.fit(X_train_sc, Y_train)
print("Best params: ", rf_RandomGrid.best_params_)
print (f'Train Accuracy: {rf_RandomGrid.score(X_train_sc,Y_train):.4f}')
print (f'Test Accuracy: {rf_RandomGrid.score(X_test_sc,Y_test):.4f}')

"""Building the final model"""

results_rf = []
final_rf = RandomForestClassifier(**rf_RandomGrid.best_params_)
final_model = final_rf.fit(X_train_sc, Y_train)

"""Predicting churn for a sample input using the final model"""

sample = [[600, 1, 0, 35, 3, 20000.00, 2, 0, 1, 200000.00]]
sample_df = pd.DataFrame(sample, columns=['credit_score', 'country', 'gender', 'age', 'tenure', 'balance'	,'products_number' ,'credit_card',	'active_member'	,'estimated_salary'])
print("Sample input:- \n")
display(sample_df)
print("\n\nPredicted value:- \n")
scaler = StandardScaler()
sample = scaler.fit_transform(sample)
final_model.predict(sample)

"""According to our model, this customer is likely to churn

Saving the model using 'pickle'
"""

filename = 'final_model.sav'
pickle.dump(final_model, open(filename, 'wb'))

"""Model has been saved"""